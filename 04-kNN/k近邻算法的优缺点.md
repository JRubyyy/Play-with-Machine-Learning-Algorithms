优点：思想极度简单，应用数学知识少，效果好，可以解释机器学习算法使用过程中的很多细节问题，更完整的刻画机器学习应用的流程。 <br><br>

最大的缺点：效率低下。 <br>
如果训练集有m个样本，n个特征，则预测每一个新的数据，都需要计算这一个数据和所有的m个样本它们之间的距离，而每计算一个距离就要使用O(n)的时间复杂度，计算m个样本的距离需要O(m*n)的时间复杂度。 <br>
优化，使用树结构：KD-Tree，Ball-Tree。即使如此，k近邻算法的效率依然非常低。 <br><br>

缺点2：高度数据相关。 <br>
对于机器学习算法来说，我们就是使用喂给机器学习算法的数据来进行预测，所以理论上所有的机器学习算法都是高度数据相关的，不过k近邻算法相对而言，对outliner更加敏感。 <br>
假设我们使用3近邻算法，但是在我们要预测的样本周边，一旦有2个错误的值的话，就足以让我们最终的预测结果变得错误，哪怕在更高的范围里，在这个空间中其实是有大量正确的样本的。 <br><br>

缺点3：预测结果不具有可解释性。 <br><br>

缺点4：维数灾难。 <br>
随着维度的增加，“看似相近”的两个点之间的距离越来越大。 <br>
而由于k近邻算法非常依赖两个点之间的距离的计算，这使得我们一旦使用k近邻算法处理高维度数据的时候，就很有可能遭受维数的灾难。 <br>
举例： <br>
| 1维 | 0到1的距离 | 1 | <br>
| :------: | :------: | :------: | <br>
| 2维 | (0,0)到(1,1)的距离 | 1.414 | <br>
| 3维 | (0,0,0)到(1,1,1)的距离 | 1.73 | <br>
| 64维 | (0,0,...,0)到(1,1,...,1)的距离 | 8 | <br>
| 10000维 | (0,0,...,0)到(1,1,...,1)的距离 | 100 | <br>
解决方法：降维。（如：PCA） <br><br>
对8*8大小的图片进行手写数字的识别，每一个数据都是64维的数据。
